{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\\n\".join(open('mioti_style.css', 'r').readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mioti.png\" style=\"height: 100px\">\n",
    "<center style=\"color:#888\">Módulo Data Science in IoT<br/>Asignatura Machine Learning</center>\n",
    "\n",
    "# Worksheet S4: Manejo de enumerados, árboles de decisión y curvas ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "Los objetivos de este worksheet son que:\n",
    "\n",
    "* Aprendas a tratar con atributos enumerados\n",
    "* Aprendas a manejar clasificadores basados en árboles de decisión\n",
    "* Comprendas el uso y significado de las curvas ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También vamos a cargar algunas funciones que nos resultarán interesantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.externals.six import StringIO  # doctest: +SKIP\n",
    "from sklearn.tree import export_graphviz\n",
    "from scipy.misc import imread\n",
    "from scipy import ndimage\n",
    "\n",
    "import re\n",
    "\n",
    "X, y = make_blobs(centers=[[0, 0], [1, 1]], random_state=61526, n_samples=50)\n",
    "\n",
    "def tree_image(decission_tree):\n",
    "    from sklearn import tree\n",
    "    import graphviz\n",
    "    dot_data = tree.export_graphviz(decission_tree, \n",
    "                                    out_file=None, \n",
    "                                    impurity=False,\n",
    "                                    filled=True,\n",
    "                                    rounded=True)\n",
    "    graph = graphviz.Source(dot_data) \n",
    "    graph.format = \"jpg\"\n",
    "    graph.render('out')\n",
    "    return imread(\"out.jpg\")\n",
    "\n",
    "\n",
    "def plot_tree(max_depth=1):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    h = 0.02\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    if max_depth != 0:\n",
    "        tree = DecisionTreeClassifier(max_depth=max_depth, random_state=1).fit(X, y)\n",
    "        Z = tree.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        faces = tree.tree_.apply(np.c_[xx.ravel(), yy.ravel()].astype(np.float32))\n",
    "        faces = faces.reshape(xx.shape)\n",
    "        border = ndimage.laplace(faces) != 0\n",
    "        ax[0].contourf(xx, yy, Z, alpha=.4)\n",
    "        ax[0].scatter(xx[border], yy[border], marker='.', s=1)\n",
    "        ax[0].set_title(\"max depth = %d\" % max_depth)\n",
    "        ax[1].imshow(tree_image(tree))\n",
    "        ax[1].axis(\"off\")\n",
    "    else:\n",
    "        ax[0].set_title(\"data set\")\n",
    "        ax[1].set_visible(False)\n",
    "    ax[0].scatter(X[:, 0], X[:, 1], c=np.array(['b', 'r'])[y], s=60)\n",
    "    ax[0].set_xlim(x_min, x_max)\n",
    "    ax[0].set_ylim(y_min, y_max)\n",
    "    ax[0].set_xticks(())\n",
    "    ax[0].set_yticks(())\n",
    "\n",
    "def plot_tree_interactive():    \n",
    "    from IPython.html.widgets import interactive, IntSlider\n",
    "    slider = IntSlider(min=0, max=8, step=1, value=0)\n",
    "    return interactive(plot_tree, max_depth=slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Para este worksheet utilizaremos un dataset clásico que es Titanic. Este dataset contiene información acerca de los supervivientes del titanic.\n",
    "\n",
    "Puedes encontrar más información del mismo en: https://www.kaggle.com/c/titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre inspeccionamos el datases antes de trabajar con él."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La descripción de cada uno de los atributos es la siguiente:\n",
    "\n",
    "```\n",
    "pclass          Passenger Class\n",
    "                (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "survival        Survival\n",
    "                (0 = No; 1 = Yes)\n",
    "name            Name\n",
    "sex             Sex\n",
    "age             Age\n",
    "sibsp           Number of Siblings/Spouses Aboard\n",
    "parch           Number of Parents/Children Aboard\n",
    "ticket          Ticket Number\n",
    "fare            Passenger Fare\n",
    "cabin           Cabin\n",
    "embarked        Port of Embarkation\n",
    "                (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "boat            Lifeboat\n",
    "body            Body Identification Number\n",
    "home.dest       Home/Destination\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable objetivo en este problema es `survived` que indica si un pasajero sobrevivió o no:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_y = df.survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A efectos de manejar esta variable en `sklearn` la vamos a convertir a un array de `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_y = np.array(df.survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el objetivo de simplificar el análisis que vamos a realizar seleccionamos las variables que pensamos que son más adecuadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df[['sex', 'pclass', 'age', 'sibsp', 'parch', 'embarked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos\n",
    "\n",
    "Como siempre, vamos a inspeccionar y estudiar cada variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sexo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.sex.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos es una variable que toma dos valores 'male' y 'female'. Como la mayor parte de los algoritmos de clasificación no son capaces de tratar con variables que no son numéricas vamos a convertir esta variable en numérica.\n",
    "\n",
    "En este caso es muy sencillo, realizaremos un `mapping` entre los valores originales y $0$ y $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_sex = {'female' : 0, 'male' : 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.sex = df_X.sex.map(map_sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el mapping ha reemplazado los valores originales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.embarked.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso esta variable tiene 3 posibles valores (S, C, Q). Aquí ya no está tan claro como realizar el paso de enumerados a variables numéricas.\n",
    "\n",
    "Uno podría pensar que hacer un `mapping` a los valores $0$, $1$ y $2$ es una solución razonable. Lo que pasa con esta aproximación es que en los números $0$, $1$ y $2$ ya estamos introduciendo una relación de orden que en los valores enumerados no existe.\n",
    "\n",
    "Es decir, la distancia entre $2$ y $0$ es mayor que la distancia entre $1$ y $0$, y esto no se da en los datos originales. Este tipo de relaciones afectan a los resultados. \n",
    "\n",
    "Otra solución, es el uso de variables ficticias. Estas variabes lo que hacen es introducir una columna por cada posible valor del enumerado que tome un valor binario. La ventaja de este enfoque es que no introducimos una relación de orden inexistente pero por contra aumentamos en gran medida el número de columnas (dimensionalidad) del conjunto de entrada y también damos pie a que los datos puedan tomar valores incongruentes.\n",
    "\n",
    "Para generar variables ficticias en pandas en tan fácil como usar la función `get_dummies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = pd.get_dummies(df_X, columns=['embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.pclass.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este caso es parecido al anterior, ya que aunque los valores son numéricos realmente se refieren a un campo de tipo enumerado \"1ª clase\", \"2ª clase\" y \"3ª clase\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = pd.get_dummies(df_X, columns=['pclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### age\n",
    "\n",
    "En este caso la edad es un valor numérico. En estos casos resulta interesante realizar un histograma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.age.plot.hist(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y analizar si tiene valores nulos o no:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.age.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X[df_X.age == np.nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tiene valores nulos, tenemos que eliminarlos, porque la mayor parte de algoritmos de clasificación no son capaces de lidiar con ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.age[df_X.age.isnull()] = df_X.age.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sibsp y parch\n",
    "\n",
    "Por último estudiamos sibsp y parch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.sibsp.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.parch.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son numéricos, no tienen nulos por lo que están OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen\n",
    "\n",
    "Hemos revisado todos los valores y los hemos transformado para que:\n",
    "\n",
    "* No haya valores nulos\n",
    "* Todos los valores sean numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último convertimos el dataset a un array de numpy para utilizarlo en `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_X = np.array(df_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunto de entrenamiento y de test\n",
    "\n",
    "Como otras veces dividiremos los datos en conjunto de entrenamiento y test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_X, dataset_y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árboles de decisión\n",
    "\n",
    "Los árboles de decisión son uno de los métodos más habituales en la toma de decisión. Su objetivo es representar en forma de árbol todas las posibles opciones de decisión.\n",
    "\n",
    "<img src=\"ejemplo_arbol.gif\" width=\"40%\"/>\n",
    "\n",
    "Las ventajas/inconvenientes de los árboles de decisión frente a otros métodos de clasificación:\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "* Aumentan su complejidad a medida que aumentan los datos\n",
    "* Se pueden entender y comprender el modelo de razonamiento*\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "* Aumentan su complejidad a medida que aumentan los datos\n",
    "* A veces tienen mucha inestabilidad. Cambiando un dato cambia el árbol completo.\n",
    "\n",
    "Podemos entender un poco mejor como funcionan con el siguiente ejemplo (para que funcione el ejemplo es preciso tener instalado el paquete `python-graphviz` y `ipywidgets`, puedes instalarlos con `conda install python-graphviz ipywidgets`):\n",
    "\n",
    "\n",
    "<div style=\"background-color:#FFFFCC; padding: 10px; margin:10px; border-radius:10px\">\n",
    "    El paquete <strong>graphviz</strong> no es compatible con sistemas operativos windows (requiere bibliotecas nativas de dibujado de gráficos que están disponibles en este sistema operativo). Si usas un sistema Linux o OSX no tendrás problema. Acuérdate de una vez instalados los paquetes reiniciar jupyter notebook.\n",
    "    \n",
    "    \n",
    "Si usas windows, no te preocupes, veremos el ejemplo en clase y es simplemente un refuerzo a la explicación. Puedes pasar a la siguiente línea.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_tree_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen multitud de algoritmos basados en árboles de clasificación, en `sklearn` podemos utilizar árboles de clasificación de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "decission_tree = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Su funcionamiento es análogo a otros algoritmos de clasificación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decission_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obtener su porcentaje de acierto de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decission_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = decission_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la matriz de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "skplt.metrics.plot_confusion_matrix(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos dibujar los árboles de decisión de la siguiente manera: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "dot_data = tree.export_graphviz(decission_tree, \n",
    "                                out_file=None, \n",
    "                                feature_names = list(df_X.columns),\n",
    "                                class_names = ['dead', 'alive'],  \n",
    "                                impurity=False,\n",
    "                                filled=True,\n",
    "                                rounded=True,) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También lo podemos guardar en PDF con:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph.render('titanic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForests\n",
    "\n",
    "Un algoritmo que da muy buenos resultados son los random forests. La idea subyacente de este algoritmo es muy sencilla: Se cogen diferentes partes de los datos y por cada una de ella se entrena una árbol. Cuando se va a evaluar una nueva instancia, se la somete a todos árboles generados y se promedian los resultados.\n",
    "\n",
    "Este algoritmo permite que entrenar distintos árboles que tengan en cuenta muy diferentes partes de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curvas ROC\n",
    "\n",
    "Un problema que tienen las matrices de confusión y todas las medidas que se derivan de ella, es que todas dependen de un valor de umbral del algoritmo. Es decir, los algoritmos de clasificación en general lo que generan es una probabilidad de pertenencia a cada una de las clases objetivo.\n",
    "\n",
    "Lo podemos ver en `sklearn` las podemos ver con:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " decission_tree.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre estas probabilidades se aplica un umbral para determinar a que clase pertenece cada instancia. Lo que pasa es que este enfoque tiene sus limitaciones.\n",
    "\n",
    "Por ejemplo tenemos una instancia que se ha clasificado de la siguiente manera:\n",
    "\n",
    "$$P(Clase_A) = 0.51\\%$$\n",
    "$$P(Clase_B) = 0.49\\%$$\n",
    "\n",
    "¿Consideramos con estos datos que tenemos un buen clasificador?\n",
    "\n",
    "Existe un criterio más potente que consiste en variar el umbral de clasificación y por cada escenario estudiar los resultados. La acumulación de esos resultados se agrega en una curva que se denomina ROC (Receiver Operating Characteristic). \n",
    "\n",
    "https://es.wikipedia.org/wiki/Curva_ROC\n",
    "\n",
    "Para calcular y dibujar estas curvas, está disponible un paquete en scikit que se llama `scikit-plot`. Puedes instalarlo con el siguiente comando: `conda install -c conda-forge scikit-plot`.\n",
    "\n",
    "Y utilizarlo de la siguente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "skplt.metrics.plot_roc(y_test, decission_tree.predict_proba(X_test))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las curvas ROC estudian la relación entre `True positive rate` o comúnmente llamada la sensibilidad: \n",
    "\n",
    "$$VPR=\\frac{VP}{P}=\\frac{VP}{(VP+FN)}$$\n",
    "\n",
    "Y el `False positive rate`:\n",
    "\n",
    "$$FPR = \\frac{FP}{N} = \\frac{FP}{(FP+VN)}$$\n",
    "\n",
    "Es una manera resumida de ver la \"capacidad de separabilidad\" del clasificador. Ya no vale que clasifique bien, sino que tiene que diferenciar \"claramente\" los casos.\n",
    "\n",
    "Una buena medida de calidad de un clasificador es el área bajo la curva (AUC) de la curva ROC. Que es un valor entre $0.5$ y $1$. Donde $0.5$ es un clasificador aleatorio y $1$ un clasificador perfecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "skplt.metrics.plot_roc(y_test, random_forest.predict_proba(X_test))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
