{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mioti.png\" style=\"height: 100px\">\n",
    "<center style=\"color:#888\">Módulo Data Science in IoT<br/>Asignatura Data preprocessing</center>\n",
    "\n",
    "# Extra worksheet S7: Introducción a Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los Word Embeddings llevan mucho tiempo existiendo. De hecho, ya hemos hablado de ellos: Bag of Words podría considerarse un Word Embedding donde una palabra o término es representada por su aparición o no aparición en los documentos del dataset. Sin embargo, y pese a que funciona muy bien para problemas pequeños y específicos tiene varios problemas:\n",
    "* Matrices gigantescas que consumen la memoria RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria: 1.28 GB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "matriz = np.random.rand(8000, 20000)\n",
    "print(\"Memoria: {} GB\".format(matriz.nbytes/10**9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bag of Words trata cada término como una unidad de información. No considera el contexto (aunque hay formas de intentarlo...).\n",
    "* Debido a esto, cada problema es extremadamente dependiente del dominio (\"temática\" que esté tratando el dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué son exactamente los Word Embeddings?\n",
    "\n",
    "No son más que representación de términos o palabras mediante arrays o vectores numéricos. Por ejemplo `\"banana\"` podría estar representada por el embedding de 2 dimensiones `[0.98, 0.02]`. Estos valores podrían significar que la palabra banana tiene un 0.90 de característica de \"fruta\" y un 0.02 de característica de \"humano\", como si se tratasen de variables de un dataset habitual.\n",
    "\n",
    "Sin embargo, **tanto el significado de estas variables como sus valores se computan automáticamente** mediante técnicas de Deep Learning, gracias a que somos capaces de capturar el contexto que rodea a cada una de las palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo aprende un algoritmo los Word Embeddings?\n",
    "\n",
    "<img src=\"model.png\" width = 800>\n",
    "\n",
    "Para aprender los Word Embeddings, hay varios elementos clave que debemos tener en cuenta:\n",
    "\n",
    "* Definimos un **vocabulario $V$** como la lista de palabras de nuestro corpus. Tamaño: `10000 palabras`. $$V = [a, aaron, abacus..., zulu]$$\n",
    "<br>\n",
    "* Cada palabra, vendrá representada por un **vector One-Hot encoding** ($o$) de tamaño `(1, longitud(V))`. P. ej: si la palabra `\"banana\"` es la palabra número 1231 de nuestro vocabulario, el vector One-Hot encoding $o_{1231}$ estará formado por 10000 elementos a valor 0, menos el de la posición 1231 que será un 1. Tamaño: `(1, 10000)`.$$\"banana\" = o_{1231} = [0, 0, 0, ..., 1, 0, ..., 0]$$\n",
    "<br>\n",
    "* **Matriz E** de valores (también llamados pesos o parámetros) que se irá \"ajustando\" al objetivo a optimizar del algoritmo. En realidad, el objetivo del algoritmo no es importante, lo importante es cómo esa matriz E se va ajustando. El tamaño de la matriz será `(longitud(V), #num. variables)`. Tamaño: `(10000, 300)`.\n",
    "<br>\n",
    "* **Vector e** para cada palabra. Esto es el word embedding. Tamaño: `(1, #num. variables)`.\n",
    "\n",
    "Imaginemos ahora un problema de clasificación donde tenemos la frase _\"I want a glass of orange ...\"_. Obviamente esperaríamos encontrarnos _\"juice\"_ como palabra más probable al final de esa frase. En este caso, estamos hablando de que _\"I want a glass of orange\"_ es el contexto (c), y _\"juice\"_ es la palabra objetivo (t).\n",
    "\n",
    "Con suerte y después de muchas iteraciones, nuestro algoritmo optimizará los pesos de la matriz E, que resultará en la obtención de nuestros Word Embeddings para todo el dataset con el que hayamos alimentado nuestro modelo. A grandes rasgos, nuestro algoritmo descubrirá que si aprende similares valores para todas las frutas, se ajustará mejor a lo que esperamos del clasificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué los hace interesantes?\n",
    "\n",
    "* La multiplicación de matrices resultará en Word Embeddings de dimensiones de: (1, 10000) * (10000, 300) = (1, 300). Esto es mucho mejor que el sistema que hemos visto en Bag of Words, donde cada palabra dependía del número de documentos y no de un número de variables fijo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria: 0.024 GB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "matriz = np.random.rand(10000, 300)\n",
    "print(\"Memoria: {} GB\".format(matriz.nbytes/10**9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gracias a ser capaces de capturar el contexto, son **capaces de generalizar** con basándose en contextos similares.\n",
    "* **Transfer learning**. Una vez se han aprendido los Word Embeddings, seremos capaces de \"transferirlos\" a otros modelos para buscar otra utilidad sin comenzar un entrenamiento \"en frío\". Esto es común en Deep Learning, pero en NLP se ha demostrado muy útil.\n",
    "* **Capaces de capturar correspondencias, analogías y hasta evolución en términos**:\n",
    "    * Man -> Woman as King -> Queen\n",
    "    * $e_{man} - e_{woman} ~= e_{king} - e_{queen}$\n",
    "    * $e_{london} ~= e_{madrid} - e_{spain} + e_{uk}$\n",
    "\n",
    "<img src=\"word_vectors.png\" width =800>\n",
    "\n",
    "<img src=\"evolving_word_embeddings.jpeg\" width = 800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desafíos surgidos\n",
    "\n",
    "Uno de los desafíos surgidos a causa de esta técnica son los \"bias\" o \"sesgos\" que se asumen de aprender de un dataset junto su contexto. Si un algoritmo es entrenado con textos machistas, los Word Embeddings representarán ese machismo.\n",
    "\n",
    "Sin embargo, hay soluciones como las que se muestran a continuación:\n",
    "\n",
    "<img src=\"wordembedding_bias.png\" width=800>\n",
    "\n",
    "1. <font color='blue'>Identificar la dimensión del bias (en este caso sexo)</font>:\n",
    "    1. Calculando la media de: ($e_{he} - e_{she}, e_{male} - e_{female}, ...$)\n",
    "    2. El resto de dimensiones las consideramos en un único eje.\n",
    "    \n",
    "2. <font color='red'>Neutralizar el sesgo para todas las palabras en la que el género no sea definitorio.</font>\n",
    "    1. \"Doctor\" y \"Nurse\" tienen género neutral (en inglés). Neutralizamos.\n",
    "    2. \"Girl\" y \"Boy\" tienen género definitorio. No neutralizamos.\n",
    "    \n",
    "3. <font color='green'>Equalizar pares de palabras a la misma distancia de dimensión Bias.</font>\n",
    "    1. \"Grandmother\" y \"Grandfather\" deberían estar ambas a la misma distancia del eje, pues un género no es mayor que otro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías y aplicaciones\n",
    "\n",
    "Las dos librerías más usadas hoy en día para realizar tareas de texto con embeddings son Gensim y Spacy. Gensim empezó siendo una libería para extraer \"topics\" de texto y ha acabado evolucionando a algo más complejo con embeddings como parte importante. Spacy es toda una suite de herramientas todas basadas en modelos de Deep Learning.\n",
    "\n",
    "*Nota: Para poder instalar modelos de spacy desde jupyter notebook, hay que ejecutar jupyter notebook con permisos de administrador del equipo.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalar requisitos\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install spacy\n",
    "# !python -m spacy download es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes técnicas son un par de ejemplos que nos ofrecen estas librerías gracias a embeddings. Es por esto que en los próximos años será el momento en el que NLP despegará como ya lo hizo la visión por computador hace un par de años."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parseo de dependencias y Parts of Speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"es\" id=\"51caaff650174c568ff32e34186d005a-0\" class=\"displacy\" width=\"750\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Esto</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">es</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">una</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">frase.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-51caaff650174c568ff32e34186d005a-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-51caaff650174c568ff32e34186d005a-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-51caaff650174c568ff32e34186d005a-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-51caaff650174c568ff32e34186d005a-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cop</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-51caaff650174c568ff32e34186d005a-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-51caaff650174c568ff32e34186d005a-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"es\")\n",
    "\n",
    "doc = nlp(u\"Esto es una frase.\")\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER: Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "Junntar 34 41 MISC\n",
      "Iván Arévalo 54 66 PER\n",
      "1/10/2019 71 80 MISC\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " estaría pensando en comprar \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Junntar\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       ", startup de \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Iván Arévalo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ", el \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    1/10/2019\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(u\"Apple estaría pensando en comprar Junntar, startup de Iván Arévalo, el 1/10/2019\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    \n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
